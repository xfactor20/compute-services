# Provider configuration
provider "google" {
  project = var.gcp_project_id
  region  = var.gcp_region
}

provider "kubernetes" {
  host                   = google_container_cluster.gke_cluster.endpoint
  cluster_ca_certificate = base64decode(google_container_cluster.gke_cluster.master_auth.0.cluster_ca_certificate)
  token                  = data.google_client_config.default.access_token

  exec {
    api_version = "client.authentication.k8s.io/v1beta1"
    command     = "gcloud"
    args        = ["config", "config-helper", "--format=json"]
  }
}

data "google_client_config" "default" {}

# VPC Network and Subnetwork Setup

# Create a VPC network
resource "google_compute_network" "gke_vpc_network" {
  name                   = "gke-vpc-network"
  auto_create_subnetworks = false
  description             = "Custom VPC network for GKE"
}

# Create a subnetwork within the VPC
resource "google_compute_subnetwork" "gke_subnetwork" {
  name          = "gke-subnetwork"
  ip_cidr_range = "10.0.0.0/16"  # Adjust the IP range as needed
  region        = var.gcp_region
  network       = google_compute_network.gke_vpc_network.id

  # Enable private Google access for GKE nodes
  private_ip_google_access = true
}

# Firewall Rules

# Allow incoming traffic to nodes from the internet (external traffic)
resource "google_compute_firewall" "allow_external_traffic" {
  name    = "allow-external-traffic"
  network = google_compute_network.gke_vpc_network.id

  # Source IP ranges (this allows all incoming traffic)
  source_ranges = ["0.0.0.0/0"]

  # Allow the following protocols and ports (adjust as needed)
  allow {
    protocol = "tcp"
    ports    = ["80", "443", "22", "8000", "11434"]  # Allow HTTP, HTTPS, SSH and Agent applications
  }
}

# Allow internal communication between nodes
resource "google_compute_firewall" "allow_internal_traffic" {
  name    = "allow-internal-traffic"
  network = google_compute_network.gke_vpc_network.id

  # Source and destination IP ranges
  source_ranges      = ["10.0.0.0/16"]  # The range of the subnetwork
  destination_ranges = ["10.0.0.0/16"]

  allow {
    protocol = "tcp"
    ports    = ["0-65535"]  # Allow all TCP ports internally
  }

  allow {
    protocol = "udp"
    ports    = ["0-65535"]  # Allow all UDP ports internally
  }

  allow {
    protocol = "icmp"
  }
}

# External IP allocation for nodes

# Allocate external IP address for the nodes
resource "google_compute_address" "external_ip" {
  name         = "node-external-ip"
  address_type = "EXTERNAL"
}

# GKE Cluster and Node Pool Configuration

# Define the GKE cluster
resource "google_container_cluster" "gke_cluster" {
  name               = "${var.gcp_project_id}-cluster"
  location           = "us-west1-a"
  initial_node_count = 1

  deletion_protection = false

  # Enable private nodes for internal IPs
  private_cluster_config {
    enable_private_nodes    = true
    enable_private_endpoint = true
    master_ipv4_cidr_block  = "172.16.0.0/28"  # Adjust this CIDR block as needed
  }

  master_authorized_networks_config {
    cidr_blocks	{ 
        cidr_block   = "0.0.0.0/0"  # Allows access from any IP, for testing only.  Restrict to IP block for use
        display_name = "Admin System"  # Adjust this for your specific network requirements
    }
  }

  network    = google_compute_network.gke_vpc_network.id
  subnetwork = google_compute_subnetwork.gke_subnetwork.name

  node_config {
    machine_type = "n1-highmem-8"
    disk_size_gb = 100  # Adjust as needed
    oauth_scopes = [
      "https://www.googleapis.com/auth/cloud-platform",
    ]

    guest_accelerator {
      type  = "nvidia-tesla-p100"
      count = 1  # One GPU per node
    }

    metadata = {
      "disable-legacy-endpoint" = "true"
    }

    # Enable GPU driver installation
    workload_metadata_config {
      mode = "GCE_METADATA"
    }
  }

  # Enable autoscaling
  remove_default_node_pool = true
}

# Define the GKE node pool with internal and external IPs
resource "google_container_node_pool" "gke_node_pool" {
  name               = "${var.gcp_project_id}-node-pool"
  cluster            = google_container_cluster.gke_cluster.name
  location           = google_container_cluster.gke_cluster.location
  initial_node_count = 1

  node_config {
    machine_type = "n1-highmem-8"
    disk_size_gb = 100  # Adjust as needed

    guest_accelerator {
      type  = "nvidia-tesla-p100"
      count = 1
    }

    oauth_scopes = [
      "https://www.googleapis.com/auth/cloud-platform",
    ]

    metadata = {
      "disable-legacy-endpoint" = "true"
    }

    # Allocate internal and external IPs
    network_config {
      create_pod_range = true
      pod_range        = google_compute_subnetwork.gke_subnetwork.ip_cidr_range  # Ensure subnet is defined
    }

    # External IP allocation for public internet access
    access_config {
      nat_ip = google_compute_address.external_ip.address
    }
  }

  # Optional autoscaling configuration
  autoscaling {
    min_node_count = 1
    max_node_count = 3
  }

  management {
    auto_upgrade = true
    auto_repair  = true
  }
}

# Apply the Kubernetes manifest using a null_resource
resource "null_resource" "apply_kubernetes_manifest" {
  depends_on = [
    google_container_cluster.gke_cluster,
    google_container_node_pool.gke_node_pool,
  ]

  provisioner "local-exec" {
    command     = "kubectl apply -f mor-chat-dply.yaml"
    environment = {
      KUBECONFIG = "${path.module}/kubeconfig"
    }
  }
}

# Generate kubeconfig file
resource "local_file" "kubeconfig" {
  content  = templatefile("${path.module}/kubeconfig.tmpl", {
    cluster_ca_certificate = google_container_cluster.gke_cluster.master_auth[0].cluster_ca_certificate
    endpoint               = google_container_cluster.gke_cluster.endpoint

    cluster_name           = google_container_cluster.gke_cluster.name
    cluster_zone           = google_container_cluster.gke_cluster.location
    project_id             = var.gcp_project_id
    token                  = data.google_client_config.default.access_token
  })
  filename = "${path.module}/kubeconfig"
}
